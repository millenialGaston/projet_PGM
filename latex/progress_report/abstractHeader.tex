\usepackage{abstract}
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Running title $\bullet$ May 2016 $\bullet$ Vol. XXI, No. 1} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section
%\input{1page_header.tex}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up
\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{Progress Report} % Article title
\author{
\textsc{Frederic Boileau}\thanks{A thank you or further information} \\[1ex] % Your name
\normalsize Universite de Montreal \\ % Your institution
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
\begin{abstract}
In our original project plan we had indicated that we wished to explore 
different network topologies to build a generative model for time series 
data, more specifically short text sequences. After parsing through the latest
literature we have opted for a RNN-LSTM network. They are several reasons for
this. The most simple option would have been to use a Hidden Markov Model
(HMM), however this topology suffers from several drawbacks, first the 
Markov assumption is unreasonably strong for most text processing and 
using an N'th order HMM is not an alternative as time complexity grows 
exponentially with N. The Recurrent Neural Network (RNN) type of neural
network allows one to introduce memory into an Artifical Neural Network (ANN)
while keeping the model tractable. However RNN's suffer from numerical issues,
most notably the vanishing-exploding gradient problem which is readily solved
using a Long-Short Term Model (LSTM). \\
\end{abstract}}

