
Two text sequences (referred to as datasets later on) were used for training:
two concatenated novels of Jane Austen and of George Eliot respectively.  All
in English and utf-8 encoded.  Punctuation and structure was left unprocessed.
Each dataset was split in sequences of 50 tokens (i.e.  ``words'') and a
dictionnary was built from the complete input ($\approx 60k$ unique tokens),
defining the input space for the networks.  Vector encoding of this space was
used through an embedding layer mapping the words to a real vector space of
dimension 256.  Available embeddings such as \textit{word2vec} and
\textit{glove} were initally used but proved to be more cumbersome than our own
trained version. 

We trained four LSTM networks with the aforementionned data, a generator and a
classifier, once without preprocessing and another where the NLTK Part of
Speech Tagger (POS tagger) was used to filter out proper names and replace them
with generic ones from the \textit{names} dataset of the NLTK library. The
generic name replacement was shared across the Jane Austen and George Eliott
datasets. This was an effort to make the classifier be trained on rather
structural aspects of the prose, such as syntax, length of sentences and broad
notions of \textit{style} as opposed to simply directly matching vocabulary
which would have been trivial with the names of the characters.


Training was achieved at ``word'' level (tokenized with NLTK).
Character level had been previsously envisionned for the rest of the project as
it is more flexible and can \textit{learn new words and structural
information}\cite{gravesGenerating} for the generators). Despite this we kept
the training at word level for multiple reasons, the primary one being
robustness towards unicode characters which might vary between versions of the
text available and the second one being speed of convergence. 

To generate the sequences the trained generators were initialized with a random
word drawn from the dictionary and the most likely next word was fed back in
the network until the desired sequence length was reached. 1000 sequences (250
per model) were generated.

The classifier were trained on the original datasets and were was then used to
classify the generated synthetic data estimate which dataset (or model,
equivalently) was used for its generation, thereby giving us some ``metric'' of
the quality of the data.

\clearpage

