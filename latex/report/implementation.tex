
Two text sequences (referred to as datasets later on) were used for training:
two concatenated novels of Jane Austen and of George Eliot respectively.  All
in English and utf-8 encoded.  Punctuation and structure was left unprocessed.
Each dataset was split in sequences of 50 tokens (i.e.  ``words'') and a
dictionnary was built from the complete input ($\approx 60k$ unique tokens),
defining the input space for the networks.  Vector encoding of this space was
used through an embedding layer mapping the words to a real vector space of
dimension 256.  Available embeddings such as \textit{word2vec} and
\textit{glove} were initally used but proved to be more cumbersome than our own
trained version. 

We trained four LSTM networks with the aforementionned data, a generator and a
classifier, once without preprocessing and another where the NLTK Part of
Speech Tagger (POS tagger) was used to filter out proper names and replace them
with generic ones from the \textit{names} dataset of the NLTK library. The
generic name replacement was shared across the Jane Austen and George Eliott
datasets. This was an effort to make the classifier be trained on rather
structural aspects of the prose, such as syntax, length of sentences and broad
notions of \textit{style} as opposed to simply directly matching vocabulary
which would have been trivial with the names of the characters.


Training was achieved at ``word'' level (tokenized with NLTK).
Character level had been previsously envisionned for the rest of the project as
it is more flexible and can \textit{learn new words and structural
information}\cite{gravesGenerating} for the generators). Despite this we kept
the training at word level for multiple reasons, the primary one being
robustness towards unicode characters which might vary between versions of the
text available and the second one being speed of convergence. 

To generate the sequences the trained generators were initialized with a random
word drawn from the dictionary and the most likely next word was fed back in
the network until the desired sequence length was reached. 1000 sequences (250
per model) were generated.

The classifier were trained on the original datasets and were was then used to
classify the generated synthetic data estimate which dataset (or model, equivalently)
was used for its generation, thereby giving us some ``metric'' of the quality
of the data.

Authorship analysis is a field of computationnal linguistics where
the author's identity is to be extracted from the writing style of text.
\cite{stylo}. It has diverse applications beyond complementing the
work of traditionnal philologists such a fraud and plagiarism detection
or even identifying cybercriminals on underground forums\cite{doppel}.
Classically the field depended on statistical analysis 
of manually engineered features such as word frequency, length of 
sentences etc. 

Despite its great importance and the fact that it is fundamentally
quite well defined, i.e. the question who wrote this text
is more well posed than the ones concerning sentiment analysis for
example, there are major challenges. Clearly one needs to differentiate
between writing features specific to writing style as opposed to
context/ content specific features as Rosenthal and Yoon mention in 
their paper on the application of stylometric techniques to detect
multiple authorship in legal decision \yrcite{rosenthal}.

In this context the simple ``anonymization'' of the text sequences we performed
has the intent to disentangle the contextual features from the imprint of the
author. We have mentionned before that the classifier could be used as some
kind of metric on the synthetic data. Conversely, suppose one is fairly
confident in the quality of the generated text, i.e. after reading its content
and using some domain knowledge experts determines that it reflects quite well
the imprint of an author's style. In this case the generated data could be a
candidate for training of classifiers to detect authorship, when the data is
sparse for instance or simply because the very nature of synthetic data could
prevent overfitting to less relevant features, the author's footprint being 
in some sense crystallized in the generator's features.


\clearpage
