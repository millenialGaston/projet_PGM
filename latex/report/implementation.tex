
Four text sequences (referred to as datasets later on) were used for training:
excerpts from Shakespeare' plays, books of the Harry Potter and Lord of the
Ring series and a list of popular quotes.  All in English and ASCII encoded.
Punctuation and structure was left unprocessed.  Each dataset was split in
sequences of 50 tokens (i.e.  ``words'') and a dictionnary was built from the
complete input ($\approx 60k$ unique tokens), defining the input space for the
networks.  Vector encoding of this space was used through an embedding layer
mapping the words to a real vector space of dimension 256.  Available
embeddings such as \textit{word2vec} and \textit{glove} were initally used but
proved to be more cumbersome than our own trained version.  Five LSTM networks
were trained with the above, Four \textit{generators} and a
\textit{classifier}. Training was achieved at ``word'' level (strings tokenized
by whitespace). Character level had been previsously envisionned for the rest
of the project as it is more flexible and can \textit{learn new words and
structural information}\cite{gravesGenerating} for the generators). Despite
this we kept the training at word level for multiple reasons, the primary one
being robustness towards unicode characters which might vary between versions
of the text available and the second one being speed of convergence. 

To generate the sequences the trained models were initialized with a random word
drawn from the dictionary and the most likely next word was fed back in the network
until the desired sequence length was reached. 1000 sequences (250 per model) were
generated. The classifier was then used to automatically estimate which
dataset (or model, equivalently) was used for its generation, thereby giving us
some ``metric'' of the quality of the data; expecting data generated from the
Shakespeare trained model to consistenly differ from the data ``generated by
Harry Potter''.
\clearpage
