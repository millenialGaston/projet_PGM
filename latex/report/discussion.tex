We first tackle the results on the text generation. It seems to be hard to draw
major conclusions from the measure that is Bit Per Character at this stage. The
preprocessing of character names doens't seem to have an interpretable effect
on the results of the generators as it gets higher for Jane Austen but lower
for George Eliot. The values for all text data tried so far were each too close
to be a solid basis for interpretation.

In the case of classification, training converges very fast, with
or without replacing character names with generic names. Moreover
it performs as well on synthetic data, meaning when trained on 
the original novels it can identify which generator output which
synthetic data. This leads us to think that the generator networks
captured quite well the defining features of the authors. Now the 
important question is whether or not those features are relevant
in determining authorship in general.

Authorship analysis is a field of computationnal linguistics where
the author's identity is to be extracted from the writing style of text.
\cite{stylo}. It has diverse applications beyond complementing the
work of traditionnal philologists such a fraud and plagiarism detection
or even identifying cybercriminals on underground forums\cite{doppel}.
Classically the field depended on statistical analysis 
of manually engineered features such as word frequency, length of 
sentences etc. 

Despite its great importance and the fact that it is fundamentally
quite well defined, i.e. the question who wrote this text
is more well posed than the ones concerning sentiment analysis for
example, there are major challenges. Clearly one needs to differentiate
between writing features specific to writing style as opposed to
context/ content specific features as Rosenthal and Yoon mention in 
their paper on the application of stylometric techniques to detect
multiple authorship in legal decision \yrcite{rosenthal}.

In this context the simple ``anonymization'' of the text sequences we performed
has the intent to disentangle the contextual features from the imprint of the
author. We have mentionned before that the classifier could be used as some
kind of metric on the synthetic data. Conversely, suppose one is fairly
confident in the quality of the generated text, i.e. after reading its content
and using some domain knowledge experts determines that it reflects quite well
the imprint of an author's style. In this case the generated data could be a
candidate for training of classifiers to detect authorship, when the data is
sparse for instance or simply because the very nature of synthetic data could
prevent overfitting to less relevant features, the author's footprint being in
some sense crystallized in the generator's features. Of course this is purely
circular if both directions are considered at the same time and we have found
the latter(using synthetic data to evaluate the generalization of the
classifier) to be more interesting both in the results it yields and in its
theoretical formulation.

In essence the goal would be to use text generation to trim from data all
contextual information that is not considered to be relevant in determining the
author (such as names or temporal and spatial markers in some cases) while
keeping the semantics of the text intact. Afterwards using that stripped text
to train the generators to obtain more synthetic data which reflects the author
in its structural semantics, beyond crude word frequency.  Of course the
question as to what to keep from the original text, what to trim and generally
what features constitute an author's footprint is one that for the moment
relies heavily on domain-knowledge, however, given that knowledge, LSTM can
leverage it to perform much deeper analyses than classical statistics provide,
relying on structure and not just isolated features.



